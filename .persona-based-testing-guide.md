# Simple-Secrets CLI â€” AI Agent QA Testing Framework v3.0

> **Testing Philosophy**: Use AI agents as intelligent QA engineers who can think creatively, discover edge cases, and simulate real user behavior that automated tests miss.

## ğŸ”— Framework Integration

This AI-driven testing framework complements systematic validation:

- **Manual Testing Framework**: See `.testing-framework.md` for systematic CLI testing checklist
- **Consistency Validation**: See `.copilot-consistency-checklist.md` for documentation sync
- **Pre-Merge Validation**: See `.pre-merge-checklist.md` for complete merge readiness

Use this framework for creative, persona-based testing that discovers edge cases automated tests miss.

## ğŸ¯ **Systematic Command Coverage Foundation**

### Phase 0: Help System Intelligence Gathering (REQUIRED FIRST)

**Goal**: Establish comprehensive command understanding before creative testing

**AI Agent Instructions:**

```text
ğŸš¨ CRITICAL: Check for environment token contamination FIRST!
Run: env | grep -i simple
If SIMPLE_SECRETS_TOKEN is set, it will auto-authenticate and mask security testing!
Either unset it or be aware it's providing authentication.

ğŸš¨ CRITICAL: Check for existing installation!
Run: ls ~/.simple-secrets
Result determines testing scenarios:

**FRESH INSTALL SCENARIO** (directory doesn't exist):
- You're testing first-run experience, setup flow, initial user creation
- Expect setup prompts, user creation wizards, initial authentication flows
- Test: setup cancellation, setup restart, invalid setup inputs

**EXISTING INSTALL SCENARIO** (directory exists):
- You're testing day-to-day operations with established users/secrets
- Expect normal authentication requirements, existing user workflows
- Test: normal CRUD operations, user management, permission scenarios

**BOTH SCENARIOS TESTING STRATEGY**:
1. First test your current scenario (existing install)
2. Then backup and remove ~/.simple-secrets for fresh install testing:
   ```bash
   cp -r ~/.simple-secrets ~/.simple-secrets.backup
   rm -rf ~/.simple-secrets
   # Test fresh install scenarios
   mv ~/.simple-secrets.backup ~/.simple-secrets  # Restore
   ```
3. Document behavior differences between scenarios

Before any creative testing, you must systematically map the application:

1. Run `./simple-secrets --help` and catalog every command listed
2. For each command, run `./simple-secrets [command] --help`
3. Document all flags, arguments, usage patterns, and examples
4. Create mental model of command relationships and workflow
5. Note any inconsistencies, missing help, or unclear documentation
6. Identify which commands might interact with each other
7. Map out potential failure points and edge cases from help alone
```

**Help System Analysis Checklist:**

- [ ] **Main help completeness**: All features mentioned in help actually work
- [ ] **Command help accuracy**: Each command's help reflects actual behavior
- [ ] **Example validity**: All examples in help work as documented
- [ ] **Flag completeness**: All mentioned flags function correctly
- [ ] **Usage pattern coverage**: All documented patterns work
- [ ] **Terminology consistency**: Same concepts use same terms throughout
- [ ] **Missing features**: Features that exist but aren't documented in help

**Command Discovery Matrix:**

Based on help system, create testing coverage map for each command with these personas:

- New User (confusion points?)
- Power User (advanced usage?)
- Malicious Actor (security concerns?)
- DevOps Engineer (automation needs?)
- Data Analyst (bulk operations?)
- Chaos Agent (unexpected combinations?)

---

## ğŸ¤– AI Agent Testing Instructions

You are a QA engineer testing this CLI application. Your goal is to:

1. **Think like a user** - both experienced and novice
2. **Think like an attacker** - try to break things
3. **Think like a developer** - understand the implementation
4. **Be creative** - find edge cases humans might miss

### Your Testing Persona Options

Assume the role of each of these and test one by one:

- ğŸ†• **New User**: First time using the app, makes common mistakes
  - **Fresh Install New User**: Never used it before, no ~/.simple-secrets directory
    - Tests first-run setup flow, auto-created config.json template, initial authentication
    - Focus: Does config.json get created? Are examples clear? Is rotation_backup_count explained?
  - **Existing Install New User**: App installed by admin, user learning to use existing system
    - Tests existing config.json usage, token storage patterns, configuration discovery
- ğŸ‘¨â€ğŸ’» **Power User**: Knows CLI tools well, uses advanced features
- ğŸ˜ˆ **Malicious Actor**: Actively trying to break security
- ğŸ”§ **DevOps Engineer**: Needs it to work in production
- ğŸ“Š **Data Analyst**: Stores lots of secrets, needs performance
- ğŸ­ **Chaos Agent**: Does unexpected, random things

## ğŸ“‹ Enhanced Interactive Testing Protocol

### Phase 1: Discovery & Learning

**Goal**: Understand the application naturally, like a real user would

```text
As an AI agent, start here:
1. Try to use the app WITHOUT reading documentation first
2. Note what's intuitive vs confusing
3. Document your learning curve
4. What assumptions did you make that were wrong?
5. What error messages were helpful vs cryptic?
```

**Questions to explore:**

- What happens if I just type `./simple-secrets`?
- Can I guess the command structure?
- Do error messages teach me how to use it correctly?
- What's my first "aha!" moment?

### ğŸ·ï¸ **Version System Discovery Testing**

**Goal**: Test version system like a user discovering it naturally

```text
Version exploration tasks:
1. How does a user discover version information?
2. Is version info consistent across commands?
3. What does version tell you about the build?
4. Does version behavior match user expectations?
```

**Discovery scenarios:**

- Try `--version`, `-v`, `version` - what works?
- What's the difference between `version` and `version --short`?
- Does version info help with debugging?
- Can you tell if you're running dev vs release build?
- What happens if you run an old binary after updating?

**Version edge cases:**

- Does version work without authentication?
- What if git isn't available during build?
- Version display with terminal width constraints
- Copy/paste version info for bug reports

### Phase 2: Destructive Creativity

**Goal**: Find bugs through creative destruction

```text
Now actively try to break things:
1. What inputs would a user NEVER intentionally provide?
2. What sequence of commands creates unexpected states?
3. Can you corrupt data through normal operations?
4. What happens during race conditions?
5. Can you exhaust resources?
```

**Creative destruction examples:**

- Put a secret with key `../../etc/passwd`
- Create circular references in values
- Use control characters, ANSI escape codes (note: null bytes are NOT a security issue - OS truncates arguments correctly)
- Rapidly rotate keys while reading secrets
- Fill the disk during a backup operation
- Delete files while the app is reading them
- Test empty arguments (note: may trigger interactive prompts - use Ctrl+C or timeout in automation)

**ğŸ›¡ï¸ Security Testing Note:**
Null byte injection (e.g., `$'key\x00'`) is NOT a security vulnerability in CLI applications. The operating system correctly truncates command line arguments at null bytes before they reach the application. This is expected and correct behavior, not a bug to report.

**ğŸš¨ CRITICAL: Shell Command Substitution Security**
Command substitution in double quotes is executed by the shell BEFORE reaching the application. This is standard shell behavior across ALL CLI tools (git, cp, mv, etc.), NOT an application vulnerability.

**DANGEROUS** (shell executes commands):
```bash
./simple-secrets put "key" "$(rm -rf /important/files)"  # Executes rm command!
git config user.name "$(rm -rf /important/files)"        # Same behavior
```

**SAFE** (literal storage):
```bash
./simple-secrets put "key" '$(rm -rf /important/files)'  # Stores literally
echo '$(dangerous command)' > file                       # Same behavior
```

**User Education Required**: Document this shell behavior in help text and examples to prevent accidental command execution during secret storage.

### ğŸ—£ï¸ **Error Message Persona Testing**

**Goal**: Test error messages from different user perspectives to ensure they're helpful, not confusing

#### The Frustrated New User Persona

```text
```text\nYou're new to CLI tools and you keep making mistakes. Every error is a learning opportunity\nor a reason to give up. Test errors through this lens:\n\n1. Do error messages teach you what to do correctly?\n2. Are technical terms explained in context?\n3. Do errors mention the help system when relevant?\n4. Can you recover from errors without panic?\n```
```

**New User Error Scenarios:**

- Try commands without reading help first
- Use wrong argument order: `./simple-secrets value key put`
- Forget authentication: just run commands without tokens
- Misspell command names: `./simple-secrets gett`, `./simple-secrets lisst`
- Use spaces wrong: `./simple-secrets put my key my value` (no quotes)
- Try Windows-style flags: `./simple-secrets get /help`

**Error Message Quality Check:**

- [ ] Does "invalid token" error explain how to get a valid token?
- [ ] Does "command not found" suggest similar valid commands?
- [ ] Does "permission denied" distinguish between file permissions vs RBAC?
- [ ] Do argument errors show exactly what was expected vs what was provided?

#### The Debugging Developer Persona

```text
You're trying to integrate this tool into scripts and automation.
Errors need to be precise and programmatically parseable.

1. Are error messages consistent in format?
2. Do exit codes match error severity?
3. Can you distinguish between temporary vs permanent failures?
4. Are error messages safe to log (no secrets leaked)?
```

**Developer Error Scenarios:**

- Capture and analyze stderr vs stdout for all commands
- Test error message stability across runs
- Verify no authentication tokens appear in error output
- Check if errors include actionable troubleshooting info
- Simulate race conditions and concurrent access errors

#### The System Administrator Persona

```text
You're managing this tool across multiple environments and users.
Errors need to help you diagnose system-wide issues.

1. Do errors indicate whether the problem is local or system-wide?
2. Can you distinguish between configuration vs data corruption?
3. Do errors suggest appropriate admin actions vs user actions?
4. Are file permission and access errors clear?
```

**Admin Error Scenarios:**

- Test with various file permission scenarios
- Simulate disk full, read-only filesystems
- Test with corrupted config files and directories
- Try running with different user privileges
- Test behavior when home directories are inaccessible

#### The Configuration-Focused User Persona

```text
You want to understand and customize configuration options.
Focus on config.json creation, token storage, and backup settings.

1. Does first-run create a helpful config.json template?
2. Are configuration examples clear and actionable?
3. Can you understand rotation_backup_count without reading docs?
4. Do configuration errors guide you toward fixes?
```

**Configuration Test Scenarios:**

- Fresh install: Does config.json get auto-created with good examples?
- Config discovery: How do you learn about available configuration options?
- Token storage: Are the three authentication methods (flag, env, config) clearly documented in config.json?
- Backup retention: Is rotation_backup_count well-explained with practical examples?
- Invalid config: What happens with malformed JSON, invalid values, wrong permissions?
- Config migration: Does upgrading preserve/migrate existing config.json settings?

#### Error Message Anti-Patterns to Check

âŒ **Never show these error patterns:**

- Raw file system paths in user-facing errors
- Stack traces or internal function names
- Cryptographic error details that confuse users
- "Installation detected" messages during normal operations
- Database or storage implementation details
- Memory addresses or internal IDs
- Generic "something went wrong" without context

âœ… **Always show these error patterns:**

- Which command/operation failed
- What the user can do to fix it
- Reference to help when appropriate
- Clear distinction between user errors vs system errors
- Consistent language for similar error types

### Phase 3: Behavioral Exploration

**Goal**: Discover undocumented behaviors and edge cases

```text
Explore the boundaries:
1. What's the largest secret you can store?
2. What's the longest key name that works?
3. How many users can you create?
4. What happens with 10,000 secrets?
5. Can you predict the token generation pattern?
```

**Behavioral tests to try:**

- Store a secret that's another encrypted secret
- Create users with emoji names
- Use the app in different locales/languages
- Run multiple instances simultaneously
- Test timezone changes during operations
- What if system time goes backwards?

## ğŸ¯ Critical Bug Hunt Scenarios

### Scenario 1: The Paranoid Security Auditor

```text
ğŸš¨ CRITICAL ENVIRONMENT CHECK FIRST:
Run: env | grep -i simple
If SIMPLE_SECRETS_TOKEN exists, unset it or testing will be contaminated!
The app will appear to work without authentication when it's actually using the env token.

You're a security auditor. Your job is to find any way that secrets
could leak or unauthorized access could occur.

Try:
- Can you access secrets without proper authentication?
- Can you escalate from reader to admin?
- Do errors reveal information about secret existence?
- Can you predict or forge tokens?
- Is there any timing attack possible?
- Can you access other users' data?
```

### Scenario 2: The Disaster Recovery Specialist

```text
You're testing disaster recovery. Assume everything that can go
wrong will go wrong at the worst possible moment.

Simulate:
- Power failure during master key rotation
- Corrupted backup files
- Partial file writes
- Missing dependencies
- Wrong file permissions
- Disk full at critical moments
- Network partition during operations
```

### Scenario 3: The Integration Nightmare

```text
You need to integrate this with existing systems that have
weird requirements and constraints.

Test:
- Secrets with JSON that breaks parsers
- Keys that conflict with environment variables
- Values that look like command injection
- Integration with CI/CD pipelines
- Secrets rotation during active use
- Migration from other secret stores
```

## ğŸ” AI-Specific Testing Advantages

### Pattern Recognition Tests

**Use your pattern recognition to find issues:**

- Do error messages follow consistent patterns?
- Are there commands that behave inconsistently?
- Can you spot potential race conditions in the workflow?
- Do you see any security anti-patterns?
- Are there performance cliffs at certain thresholds?

### Natural Language Exploitation

**Use your language understanding:**

- Create secrets with multilingual content
- Use homoglyphs and lookalike characters
- Test with RTL (right-to-left) text
- Use zero-width characters
- Test with various encodings

### Sequence Prediction

**Use your sequence learning:**

- Can you predict the next token value?
- Is there a pattern in backup naming?
- Can you guess file locations?
- Are there predictable temporary files?

## ğŸ“Š Intelligent Reporting Format

### For each testing session, report

#### 1. Executive Summary

```text
ğŸ­ Testing Persona Used: [Which persona(s)]
ğŸ¯ Focus Area: [What you concentrated on]
â±ï¸ Test Duration: [Simulated time]
ğŸ” Coverage: [What percentage of features touched]
```

#### 2. Discovered Issues (Prioritized)

```text
ğŸš¨ CRITICAL: [Security vulnerabilities, data loss risks]
âš ï¸ HIGH: [Functional bugs, performance issues]
âš¡ MEDIUM: [UX problems, minor bugs]
ğŸ’¡ LOW: [Suggestions, nice-to-haves]
```

#### 3. Creative Edge Cases Found

```text
ğŸ˜ˆ "I tried X, expecting Y, but got Z instead..."
ğŸª "This weird sequence of commands causes..."
ğŸ”¬ "Under these specific conditions..."
```

#### 4. User Experience Insights

```text
ğŸ˜• Confusing: [What wasn't intuitive]
ğŸ˜Š Delightful: [What worked really well]
ğŸ¤” Surprising: [Unexpected behaviors]
ğŸ’­ Missing: [Features users would expect]
```

#### 5. Security Observations

```text
ğŸ”’ Strong: [Good security practices noticed]
ğŸ”“ Weak: [Potential vulnerabilities]
â“ Unclear: [Security boundaries that need clarification]
```

## ğŸ® Gamified Testing Challenges

### Challenge 1: "Speedrun Any% Corruption"

How quickly can you corrupt the database from a fresh install?

### Challenge 2: "Secret Hoarder"

What's the maximum number of secrets you can create before something breaks?

### Challenge 3: "Token Collector"

How many different authentication states can you create?

### Challenge 4: "Time Traveler"

What breaks if you manipulate system time during operations?

### Challenge 5: "The Minimalist"

What's the minimum viable setup that still works?

### Challenge 6: "The Maximalist"

How complex can you make the setup while maintaining functionality?

## ğŸ§  AI Agent Testing Advantages

### What AI Agents Do Better Than Scripts

1. **Creative Thinking**
   - Generate novel test cases on the fly
   - Combine features in unexpected ways
   - Think of "what if" scenarios scripts can't

2. **Pattern Recognition**
   - Spot inconsistencies across commands
   - Identify security anti-patterns
   - Notice performance degradation patterns

3. **Natural Language Understanding**
   - Test with realistic human inputs
   - Understand error message quality
   - Evaluate documentation clarity

4. **Adaptive Testing**
   - Adjust testing based on discoveries
   - Go deeper into problem areas
   - Skip irrelevant tests intelligently

5. **User Empathy**
   - Simulate real user confusion
   - Predict common mistakes
   - Evaluate emotional response to errors

## ğŸ”„ Iterative Testing Cycles

### Cycle 1: Innocent Explorer

- Use the app as a complete newcomer
- Document the learning journey
- Note first impressions

### Cycle 2: Power User

- Use advanced features
- Chain complex operations
- Test performance limits

### Cycle 3: Hostile Actor

- Try to break security
- Attempt privilege escalation
- Look for information leaks

### Cycle 4: Production Simulator

- Test real-world scenarios
- Simulate production load
- Test failure recovery

### Cycle 5: Integration Tester

- Test with other tools
- Verify scripting compatibility
- Check ecosystem fit

## ğŸ“ˆ Testing Evolution

### How to make each session better

1. **Build on Previous Sessions**
   - Reference earlier discoveries
   - Test if previous issues are fixed
   - Go deeper into problem areas

2. **Vary Testing Personas**
   - Switch perspectives each session
   - Combine multiple personas
   - Create new personas based on findings

3. **Increase Complexity**
   - Start simple, add complexity
   - Combine multiple issues
   - Create compound failure scenarios

4. **Document Patterns**
   - Track recurring issues
   - Note design patterns (good and bad)
   - Build institutional knowledge

## ğŸ¯ Success Metrics for AI Agent Testing

### Quality Indicators

- ğŸ¨ **Creativity**: Found issues automated tests missed
- ğŸ” **Depth**: Went beyond surface-level testing
- ğŸ§© **Connections**: Found interaction bugs
- ğŸ’¡ **Insights**: Provided valuable UX feedback
- ğŸš¨ **Severity**: Found critical issues early

### Session Effectiveness

- Found at least 3 issues scripts wouldn't catch
- Discovered 1 creative edge case
- Provided 5 UX improvement suggestions
- Tested 3 scenarios not in documentation
- Simulated 2 real-world failure modes

## ğŸ¤ Collaboration with Human Testers

### AI Agent Strengths

- Tireless exploration
- Creative edge case generation
- Pattern recognition
- Never gets bored with repetitive tests
- Can simulate multiple personas quickly

### Human Tester Strengths

- Deep domain knowledge
- Business logic understanding
- Real-world experience
- Intuition about what matters
- Strategic test planning

### Best Practice

Use AI agents for exploration and creative testing, then have humans verify critical findings and make strategic decisions about what matters most.

---

## ğŸš€ Example AI Agent Testing Session Starter

```text
Hello AI Agent! You're about to test the simple-secrets CLI application.

Today's Testing Mission:
1. Persona: You're a paranoid DevOps engineer who doesn't trust anything
2. Focus: Try to corrupt data through normal-looking operations
3. Special attention: The backup/restore system
4. Bonus: Find a way to make the app consume excessive resources

Start with a fresh install and document everything you discover.
Be creative, be thorough, be adversarial.

Remember: You're not just running tests, you're thinking like a human QA engineer who's had too much coffee and really wants to find bugs.

Begin!
```

---

**Remember**: The value of AI agent testing isn't in automation - it's in intelligent, creative exploration that discovers what automated tests never would. Think outside the test case! ğŸ­
