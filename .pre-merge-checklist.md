# Pre## üîÑ Restart Directi## üîÑ Restart Directive (PERMANENT RULE)

**CRITICAL**: After any substantial changes during the validation process (such as authentication system modernization, architectural refactoring, security vulnerability fixes, or significant code restructuring), the AI assistant MUST restart this entire checklist from Phase 1, Step 1.0. (PERMANENT RULE)

**CRITICAL**: After any substantial changes during the validation process (such as authentication system modernization, architectural refactoring, security vulnerability fixes, or significant code restructuring), the AI assistant MUST restart this entire checklist from Phase 1, Step 1.0.

**Rationale**: Substantial changes can introduce new issues, affect multiple subsystems, or invalidate previous validation steps. A complete restart ensures:

- All validation steps account for the latest changes
- No validation gaps from incremental checking
- Comprehensive quality assurance after architectural modifications
- Fresh evaluation of the complete system state

**Examples of changes requiring restart**:

- Authentication system modernization
- Major refactoring of core components
- Security vulnerability remediation
- API or CLI interface changes
- Cross-cutting architectural improvements

## üìã Pre-Merge Validation Sequence

### Phase 1: Code Quality & SOLID Reviewon Checklist

## üéØ Purpose

This checklist ensures comprehensive validation before merging code changes. It consolidates multiple testing frameworks and quality checks into a single, repeatable process that guarantees merge-readiness.

## üîÑ Restart Directive (PERMANENT RULE)

**CRITICAL**: After any substantial changes during the validation process (such as authentication system modernization, architectural refactoring, security vulnerability fixes, or significant code restructuring), the AI assistant MUST restart this entire checklist from Phase 1, Step 1.0.

**Rationale**: Substantial changes can introduce new issues, affect multiple subsystems, or invalidate previous validation steps. A complete restart ensures:

- All validation steps account for the latest changes
- No validation gaps from incremental checking
- Comprehensive quality assurance after architectural modifications
- Fresh evaluation of the complete system state

**Examples of changes requiring restart**:

- Authentication system modernization
- Major refactoring of core components
- Security vulnerability remediation
- API or CLI interface changes
- Cross-cutting architectural improvements

### Phase 1: Code Quality & SOLID Review

### Pre-Validation Setup

ÔøΩ **Environment Token Check**:

```bash
env | grep -i simple
# Note: SIMPLE_SECRETS_TOKEN is often set from previous testing - this is expected.
# Just be aware it will auto-authenticate during testing.
```

üö® **Installation State Check**:

```bash
ls ~/.simple-secrets
# If directory exists, you're seeing existing installation state
# ALWAYS test fresh installation by using isolated environment:
# export SIMPLE_SECRETS_CONFIG_DIR=/tmp/test-clean && rm -rf /tmp/test-clean
# Fresh installation testing is REQUIRED regardless of existing installations
```

**Purpose**: Early validation of code architecture, clean code principles, and merge-readiness

#### 1.0 Test Suite Validation (CRITICAL FIRST STEP)

- ‚úÖ **Build Verification**: `make clean && make dev` succeeds with current commit hash
  - ‚ö†Ô∏è **NEVER use `go build` directly** - always use `make dev` for proper version injection
  - ‚úÖ **Version Check**: `./simple-secrets version` shows proper commit hash (not "unknown")
  - ‚úÖ **Short Version**: `./simple-secrets version --short` shows `dev-abc1234` format
- ‚úÖ **Test Execution**: `make test` runs with 100% pass rate (0 failed tests)
- ‚úÖ **Test Coverage**: Core functionality covered by unit and integration tests
- ‚úÖ **Concurrency Tests**: All concurrent operations tests pass without data loss
- ‚úÖ **Performance Tests**: No regressions in test execution time
- ‚ö†Ô∏è **MERGE BLOCKER**: Any test failures must be fixed before proceeding

#### 1.1 SOLID Principles Review

- ‚úÖ **Single Responsibility**: Each function/struct has one clear purpose
- ‚úÖ **Open/Closed**: Code is extensible without modification
- ‚úÖ **Liskov Substitution**: Interfaces are properly implemented
- ‚úÖ **Interface Segregation**: Interfaces are focused and minimal
- ‚úÖ **Dependency Inversion**: Dependencies are properly abstracted

#### 1.2 Clean Code Validation

- ‚úÖ **Method Extraction**: Complex logic broken into meaningful methods
- ‚úÖ **Variable Naming**: Names express intent clearly
- ‚úÖ **Abstraction Levels**: Methods operate at consistent abstraction levels
- ‚úÖ **Early Returns**: Avoid `else` branches, use guard clauses
- ‚úÖ **Error Handling**: Clear, actionable error messages
- ‚úÖ **Comments**: Minimal, only where absolutely necessary

### **1.3 Architecture & Design Validation**

- [ ] **Domain-driven structure**: `pkg/` contains public interfaces, `internal/` has implementations
- [ ] **Service composition**: `internal/platform/` properly wires all domain services together
- [ ] **Interface boundaries**: Clean domain boundaries between secrets, users, auth, rotation
- [ ] **Platform readiness**: Business logic accessible through public interfaces for API extension
- [ ] **Context integration**: All domain operations accept and properly handle `context.Context`
- [ ] **Repository pattern**: Storage abstracted behind domain-appropriate interfaces
- [ ] **Error handling**: Consistent domain error types with proper wrapping and context
- [ ] **Resource management**: Proper cleanup of resources with context cancellation support
- [ ] **Configuration handling**: Platform configuration composable and injectable
- [ ] **Dependency injection**: Platform services composed via factory pattern with options

---

### Phase 2: Consistency Validation

**Purpose**: Ensures all documentation, tests, code, and examples remain synchronized after changes
**Reference**: `.copilot-consistency-checklist.md`

Execute the most thorough consistency validation possible covering:

#### 2.1 Pre-Change Analysis (Comprehensive)

- ‚úÖ **Change Scope Documentation**: Document every file modified, added, or deleted
- ‚úÖ **Impact Analysis**: Identify ALL files that reference changed functionality
- ‚úÖ **Documentation Dependencies**: Map which docs, examples, and tests depend on changes
- ‚úÖ **Version Implications**: Assess if changes affect version numbering or compatibility
- ‚úÖ **Breaking Change Detection**: Identify any API, CLI, or behavior changes

#### 2.2 Documentation Synchronization (Exhaustive)

- ‚úÖ **README.md**: Every feature mentioned works exactly as described
- ‚úÖ **docs/getting-started.md**: Step-by-step guide reflects current CLI behavior
- ‚úÖ **CLI Help Text**: `--help` output matches actual functionality for every command
- ‚úÖ **Version Information**: All version references are current and consistent
- ‚úÖ **Code Examples**: Every example in docs actually executes successfully
- ‚úÖ **Security Documentation**: Current security model and recommendations
- ‚úÖ **Installation Instructions**: Work on clean systems with specified requirements
- ‚úÖ **Configuration Documentation**: All options documented with current defaults
- ‚úÖ **Error Message Documentation**: Error scenarios match actual error text
- ‚úÖ **Feature Flags**: Any conditional features properly documented

#### 2.3 Cross-Reference Validation (Complete)

- ‚úÖ **Code-to-Docs Mapping**: Every public function/command documented somewhere
- ‚úÖ **Example Verification**: Execute every code example in documentation
- ‚úÖ **Link Validation**: All internal and external links work
- ‚úÖ **Image/Diagram Currency**: Screenshots and diagrams reflect current UI/behavior
- ‚úÖ **Test-to-Feature Mapping**: Every documented feature has corresponding tests
- ‚úÖ **Configuration Consistency**: Default values match between code and docs
- ‚úÖ **Command Reference**: CLI reference matches `cobra` command definitions
- ‚úÖ **API Consistency**: Internal APIs match their usage patterns

#### 2.4 CLI Command Coverage Validation

- ‚úÖ **Help System Audit**: `./app --help` ‚Üí verify all commands listed in testing framework
- ‚úÖ **Command Completeness**: Every command in help has corresponding test section
- ‚úÖ **Subcommand Coverage**: Check `[command] --help` for all subcommands/flags
- ‚úÖ **Alias Testing**: Verify all command aliases (e.g., `add` ‚Üí `put`) are tested
- ‚úÖ **Flag Coverage**: All global flags (`--version`, `-v`, `--help`, `--token`) tested
- ‚úÖ **Command Variants**: All usage patterns and argument combinations covered
- ‚úÖ **Error Scenarios**: Invalid commands, missing args, wrong syntax all tested

#### 2.5 Help System Consistency Validation (CRITICAL)

- ‚úÖ **Help-to-Implementation Accuracy**: Every flag/option in help actually works as described
- ‚úÖ **Example Execution**: All examples in help text execute successfully with expected results
- ‚úÖ **Documentation Alignment**: README examples match help examples exactly
- ‚úÖ **Flag Consistency**: Same flags described consistently across all commands
- ‚úÖ **Terminology Consistency**: Same terms used consistently throughout help system
- ‚úÖ **Feature Coverage**: All implemented features mentioned in appropriate help sections
- ‚úÖ **Testing Framework Sync**: Help system changes reflected in testing framework command inventory

#### 2.6 Unit Test Coverage Validation

- ‚úÖ **Test-to-Code Mapping**: Every public function has corresponding unit test
- ‚úÖ **Critical Path Coverage**: All error handling paths have test coverage
- ‚úÖ **Edge Case Testing**: Boundary conditions, null inputs, empty values tested
- ‚úÖ **Mock Validation**: External dependencies properly mocked and tested
- ‚úÖ **Integration Points**: All module interactions have integration tests
- ‚úÖ **Regression Coverage**: Previously fixed bugs have regression tests
- ‚úÖ **Performance Tests**: Critical operations have performance validation

#### 2.7 Quality Standards Validation

- ‚úÖ **Documentation Quality**: Clear, actionable, error-free writing
- ‚úÖ **Example Quality**: Examples are realistic, useful, and current
- ‚úÖ **User Experience**: Documentation flows logically for intended audience
- ‚úÖ **Accessibility**: Documentation is clear for users at different skill levels
- ‚úÖ **Completeness**: No gaps between what code does and what docs say

**Expected Outcome**: Perfect synchronization between code, documentation, examples, and tests

---

### Phase 3: Persona-Based Testing Framework

**Purpose**: AI simulating different user personas testing their perception and experience with the application
**Reference**: `.persona-based-testing-guide.md`

Execute the complete persona-based testing framework covering:

- ‚úÖ **Discovery & Learning Phase**: New user first-time experience testing
- ‚úÖ **Destructive Creativity Phase**: Active attempts to break functionality
- ‚úÖ **Behavioral Exploration Phase**: Boundary testing and edge case discovery
- ‚úÖ **Critical Bug Hunt Scenarios**: Security auditor, disaster recovery, integration testing
- ‚úÖ **All Testing Personas**: New User, Power User, Malicious Actor, DevOps Engineer, Data Analyst, Chaos Agent
- ‚úÖ **AI-Specific Testing**: Pattern recognition, natural language exploitation, sequence prediction
- ‚úÖ **Gamified Challenges**: Speedrun corruption, secret hoarder, token collector, time traveler, minimalist, maximalist
- ‚úÖ **Iterative Testing Cycles**: Innocent explorer ‚Üí Power user ‚Üí Hostile actor ‚Üí Production simulator ‚Üí Integration tester
- ‚úÖ **Creative Edge Cases**: Multi-persona combinations, compound failure scenarios, interaction bugs

**Expected Outcome**: All personas tested, creative edge cases discovered, no critical security or functionality issues, concurrent operations validated

---

### Phase 4: Standard Testing Framework

**Purpose**: AI systematically executes the comprehensive manual testing checklist for semantic correctness and real-world functionality validation
**Reference**: `.testing-framework.md`

Execute the complete testing framework covering:

- ‚úÖ Build & Installation Testing (dev builds, version validation)
- ‚úÖ Initial Setup & First Run Protection
- ‚úÖ Authentication & Authorization (token validation, RBAC)
- ‚úÖ Secret Management (CRUD operations, large data, special characters)
- ‚úÖ User Management & Role Enforcement
- ‚úÖ Master Key Rotation & Backup Integrity (critical focus area)
- ‚úÖ Backup & Restore Operations
- ‚úÖ Consolidated Commands & Legacy Compatibility
- ‚úÖ Error Handling & Edge Cases
- ‚úÖ Performance & Load Testing
- ‚úÖ **Concurrent Operations Testing** (file locking, race conditions, data integrity)

**Expected Outcome**: All manual testing checklist items pass with no critical issues

---

### Phase 5: Cross-Reference Validation

**Purpose**: Ensure all framework files are properly integrated and documented

- ‚úÖ **Framework Integration**: All testing frameworks reference each other appropriately
- ‚úÖ **Documentation Cross-References**: Files properly link to related frameworks
- ‚úÖ **Copilot Instructions**: `.github/copilot-instructions.md` explains usage of all frameworks
- ‚úÖ **Usage Examples**: Clear examples of how to execute each framework

**Expected Outcome**: Complete, integrated documentation ecosystem

---

## üöÄ Execution Instructions

## Usage Instructions

### For AI Assistants

When user says "run pre-merge checklist" or references this file:

1. **Load Context**: Read all referenced framework files
2. **Execute Sequentially**: Run each phase in order, don't skip steps
3. **Report Results**: Provide clear pass/fail status for each phase
4. **Block on Failures**: Don't proceed to next phase if critical issues found
5. **Final Recommendation**: Clear go/no-go decision for merge

### For Human Developers

```bash
# Quick validation commands
make test                    # Unit tests
make integration-test        # Integration tests
make dev                     # Verify build and version
./simple-secrets --help      # Verify CLI functionality
```

## üìÅ Related Files

- **Testing Frameworks**:
  - `.persona-based-testing-guide.md` - AI simulating user personas and perceptions
  - `.testing-framework.md` - AI simulating rigorous human testing for semantic correctness
  - `.copilot-consistency-checklist.md` - Documentation synchronization

- **Code Guidelines**:
  - `.github/copilot-instructions.md` - Coding standards and AI guidance
  - `VERSION_GUIDE.md` - Versioning and release process

- **Development Tools**:
  - `Makefile` - Build and test automation
  - `TODO.md` - Development tracking

## üéØ Success Criteria

**Merge is approved when:**

- ‚úÖ All persona-based testing framework sections pass
- ‚úÖ All standard tests pass with no regressions
- ‚úÖ Concurrent operations validated (no race conditions or data loss)
- ‚úÖ Documentation is fully synchronized and consistent
- ‚úÖ Code follows SOLID principles and clean code standards
- ‚úÖ All framework files are cross-referenced and integrated
- ‚úÖ No critical issues or technical debt introduced

**Merge is blocked when:**

- ‚ùå Any test failures in critical functionality
- ‚ùå Documentation inconsistencies or gaps
- ‚ùå Code quality violations or technical debt
- ‚ùå Missing test coverage for new functionality
- ‚ùå SOLID principle violations or poor organization

---

*This checklist ensures that every merge maintains the highest standards of quality, consistency, and maintainability.*
